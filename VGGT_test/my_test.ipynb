{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGGT test for multipleView video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 10 Images:\n",
      "../collected_frames/images/cam10_frame_00001.jpg\n",
      "../collected_frames/images/cam9_frame_00001.jpg\n",
      "../collected_frames/images/cam8_frame_00001.jpg\n",
      "../collected_frames/images/cam7_frame_00001.jpg\n",
      "../collected_frames/images/cam6_frame_00001.jpg\n",
      "../collected_frames/images/cam5_frame_00001.jpg\n",
      "../collected_frames/images/cam4_frame_00001.jpg\n",
      "../collected_frames/images/cam3_frame_00001.jpg\n",
      "../collected_frames/images/cam2_frame_00001.jpg\n",
      "../collected_frames/images/cam1_frame_00001.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def get_image_files(folder_path, extensions=['.jpg', '.jpeg', '.png', '.gif', '.bmp']):\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if any(file.lower().endswith(ext) for ext in extensions):\n",
    "                relative_path = os.path.relpath(os.path.join(root, file), folder_path)\n",
    "                full_path = os.path.join(folder_path, relative_path)\n",
    "                file_paths.append(full_path)\n",
    "    return file_paths\n",
    "images_folder = \"../collected_frames/\"\n",
    "\n",
    "if os.path.exists(images_folder):\n",
    "        image_files = get_image_files(images_folder)\n",
    "        print(f\"\\nFound {len(image_files)} Images:\")\n",
    "        #print(image_files)\n",
    "        for img_path in image_files:\n",
    "            print(img_path)\n",
    "            \n",
    "else:\n",
    "        print(f\"Folder {images_folder} doesn't exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using the VGGT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from vggt.models.vggt import VGGT\n",
    "from vggt.utils.load_fn import load_and_preprocess_images\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "dtype = torch.float16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "def write_cameras_bin(cameras, output_path):\n",
    "    \"\"\"写入 cameras.bin 文件\"\"\"\n",
    "    with open(output_path, 'wb') as f:\n",
    "        f.write(struct.pack('<Q', len(cameras)))\n",
    "        \n",
    "        for camera_id, camera in cameras.items():\n",
    "            f.write(struct.pack('<I', camera_id))\n",
    "            f.write(struct.pack('<i', camera['model']))  # 1=PINHOLE\n",
    "            f.write(struct.pack('<Q', camera['width']))\n",
    "            f.write(struct.pack('<Q', camera['height']))\n",
    "            \n",
    "            for param in camera['params']:\n",
    "                f.write(struct.pack('<d', param))\n",
    "\n",
    "def write_images_bin(images, output_path):\n",
    "    \"\"\"写入 images.bin 文件\"\"\"\n",
    "    with open(output_path, 'wb') as f:\n",
    "        f.write(struct.pack('<Q', len(images)))\n",
    "        \n",
    "        for image_id, image in images.items():\n",
    "            f.write(struct.pack('<I', image_id))\n",
    "            \n",
    "            # 四元数 (qw, qx, qy, qz)\n",
    "            for q in image['quat']:\n",
    "                f.write(struct.pack('<d', q))\n",
    "            \n",
    "            # 平移向量 (tx, ty, tz)\n",
    "            for t in image['trans']:\n",
    "                f.write(struct.pack('<d', t))\n",
    "            \n",
    "            f.write(struct.pack('<I', image['camera_id']))\n",
    "            \n",
    "            # 图像名称\n",
    "            name_bytes = image['name'].encode('utf-8') + b'\\0'\n",
    "            f.write(name_bytes)\n",
    "            \n",
    "            # 2D点 (暂时为空)\n",
    "            f.write(struct.pack('<Q', 0))\n",
    "\n",
    "def write_points3d_bin(points3d, output_path):\n",
    "    \"\"\"写入 points3D.bin 文件\"\"\"\n",
    "    with open(output_path, 'wb') as f:\n",
    "        f.write(struct.pack('<Q', len(points3d)))\n",
    "        \n",
    "        for point_id, point in points3d.items():\n",
    "            f.write(struct.pack('<Q', point_id))\n",
    "            \n",
    "            # 3D坐标\n",
    "            for coord in point['xyz']:\n",
    "                f.write(struct.pack('<d', coord))\n",
    "            \n",
    "            # RGB颜色\n",
    "            for color in point['rgb']:\n",
    "                f.write(struct.pack('<B', color))\n",
    "            \n",
    "            f.write(struct.pack('<d', point['error']))\n",
    "            \n",
    "            # 观测信息 (暂时为空)\n",
    "            f.write(struct.pack('<Q', 0))\n",
    "\n",
    "def rotation_matrix_to_quaternion(R):\n",
    "    \"\"\"将旋转矩阵转换为四元数 (qw, qx, qy, qz)\"\"\"\n",
    "    R = R.cpu().numpy() if torch.is_tensor(R) else R\n",
    "    \n",
    "    trace = np.trace(R)\n",
    "    if trace > 0:\n",
    "        s = np.sqrt(trace + 1.0) * 2\n",
    "        qw = 0.25 * s\n",
    "        qx = (R[2, 1] - R[1, 2]) / s\n",
    "        qy = (R[0, 2] - R[2, 0]) / s\n",
    "        qz = (R[1, 0] - R[0, 1]) / s\n",
    "    else:\n",
    "        if R[0, 0] > R[1, 1] and R[0, 0] > R[2, 2]:\n",
    "            s = np.sqrt(1.0 + R[0, 0] - R[1, 1] - R[2, 2]) * 2\n",
    "            qw = (R[2, 1] - R[1, 2]) / s\n",
    "            qx = 0.25 * s\n",
    "            qy = (R[0, 1] + R[1, 0]) / s\n",
    "            qz = (R[0, 2] + R[2, 0]) / s\n",
    "        elif R[1, 1] > R[2, 2]:\n",
    "            s = np.sqrt(1.0 + R[1, 1] - R[0, 0] - R[2, 2]) * 2\n",
    "            qw = (R[0, 2] - R[2, 0]) / s\n",
    "            qx = (R[0, 1] + R[1, 0]) / s\n",
    "            qy = 0.25 * s\n",
    "            qz = (R[1, 2] + R[2, 1]) / s\n",
    "        else:\n",
    "            s = np.sqrt(1.0 + R[2, 2] - R[0, 0] - R[1, 1]) * 2\n",
    "            qw = (R[1, 0] - R[0, 1]) / s\n",
    "            qx = (R[0, 2] + R[2, 0]) / s\n",
    "            qy = (R[1, 2] + R[2, 1]) / s\n",
    "            qz = 0.25 * s\n",
    "    \n",
    "    return np.array([qw, qx, qy, qz])\n",
    "\n",
    "def convert_vggt_to_colmap(extrinsic, intrinsic, point_map_by_unprojection, \n",
    "                          depth_conf, image_files, output_dir):\n",
    "    \"\"\"将VGGT输出转换为COLMAP格式\"\"\"\n",
    "    \n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # 转换为numpy\n",
    "    if torch.is_tensor(extrinsic):\n",
    "        extrinsic = extrinsic.cpu().numpy()\n",
    "    if torch.is_tensor(intrinsic):\n",
    "        intrinsic = intrinsic.cpu().numpy()\n",
    "    if torch.is_tensor(point_map_by_unprojection):\n",
    "        point_map_by_unprojection = point_map_by_unprojection.cpu().numpy()\n",
    "    if torch.is_tensor(depth_conf):\n",
    "        depth_conf = depth_conf.cpu().numpy()\n",
    "    \n",
    "    num_images = len(image_files)\n",
    "    \n",
    "    # 1. 准备相机数据\n",
    "    cameras = {}\n",
    "    for i in range(num_images):\n",
    "        camera_id = i + 1\n",
    "        K = intrinsic[i]  # 3x3 内参矩阵\n",
    "        \n",
    "        cameras[camera_id] = {\n",
    "            'model': 1,  # PINHOLE\n",
    "            'width': point_map_by_unprojection.shape[-1],  # 图像宽度\n",
    "            'height': point_map_by_unprojection.shape[-2], # 图像高度\n",
    "            'params': [K[0, 0], K[1, 1], K[0, 2], K[1, 2]]  # fx, fy, cx, cy\n",
    "        }\n",
    "    \n",
    "    # 2. 准备图像数据 (相机位姿)\n",
    "    images = {}\n",
    "    for i in range(num_images):\n",
    "        image_id = i + 1\n",
    "        \n",
    "        # 外参矩阵 (4x4)\n",
    "        ext = extrinsic[i]\n",
    "        R = ext[:3, :3]  # 旋转矩阵\n",
    "        t = ext[:3, 3]   # 平移向量\n",
    "        \n",
    "        # COLMAP使用 world-to-camera 变换\n",
    "        # 如果VGGT输出的是camera-to-world，需要求逆\n",
    "        # 这里假设VGGT输出的是camera-to-world，所以需要求逆\n",
    "        R_inv = R.T\n",
    "        t_inv = -R_inv @ t\n",
    "        \n",
    "        # 转换为四元数\n",
    "        quat = rotation_matrix_to_quaternion(R_inv)\n",
    "        \n",
    "        images[image_id] = {\n",
    "            'quat': quat.tolist(),\n",
    "            'trans': t_inv.tolist(),\n",
    "            'camera_id': camera_id,\n",
    "            'name': Path(image_files[i]).name\n",
    "        }\n",
    "    \n",
    "    # 3. 准备3D点数据\n",
    "    points3d = {}\n",
    "    point_id = 1\n",
    "    \n",
    "    # 从point_map_by_unprojection提取3D点\n",
    "    # 形状应该是 [num_images, 3, height, width]\n",
    "    confidence_threshold = 0.5  # 置信度阈值\n",
    "    \n",
    "    for img_idx in range(num_images):\n",
    "        points_3d = point_map_by_unprojection[img_idx]  # [3, H, W]\n",
    "        conf = depth_conf[img_idx] if depth_conf is not None else np.ones_like(points_3d[0])\n",
    "        \n",
    "        # 采样点 (避免点太多)\n",
    "        H, W = points_3d.shape[1], points_3d.shape[2]\n",
    "        step = max(1, min(H, W) // 100)  # 采样步长\n",
    "        \n",
    "        for y in range(0, H, step):\n",
    "            for x in range(0, W, step):\n",
    "                if conf[y, x] > confidence_threshold:\n",
    "                    xyz = points_3d[:, y, x]\n",
    "                    \n",
    "                    # 检查点是否有效\n",
    "                    if not np.any(np.isnan(xyz)) and not np.any(np.isinf(xyz)):\n",
    "                        points3d[point_id] = {\n",
    "                            'xyz': xyz.tolist(),\n",
    "                            'rgb': [128, 128, 128],  # 默认灰色\n",
    "                            'error': 0.0\n",
    "                        }\n",
    "                        point_id += 1\n",
    "    \n",
    "    # 4. 写入文件\n",
    "    write_cameras_bin(cameras, output_dir / 'cameras.bin')\n",
    "    write_images_bin(images, output_dir / 'images.bin')\n",
    "    write_points3d_bin(points3d, output_dir / 'points3D.bin')\n",
    "    \n",
    "    print(f\"COLMAP文件已保存到: {output_dir}\")\n",
    "    print(f\"- cameras.bin: {len(cameras)} 个相机\")\n",
    "    print(f\"- images.bin: {len(images)} 张图像\")\n",
    "    print(f\"- points3D.bin: {len(points3d)} 个3D点\")\n",
    "    \n",
    "    return output_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'torch.Tensor'>\n",
      "Shape: torch.Size([10, 3, 518, 518])\n",
      "COLMAP文件已保存到: colmap_output\n",
      "- cameras.bin: 10 个相机\n",
      "- images.bin: 10 张图像\n",
      "- points3D.bin: 15540 个3D点\n"
     ]
    }
   ],
   "source": [
    "from vggt.utils.pose_enc import pose_encoding_to_extri_intri\n",
    "from vggt.utils.geometry import unproject_depth_map_to_point_map\n",
    "model = VGGT.from_pretrained(\"facebook/VGGT-1B\").to(device) \n",
    "images = load_and_preprocess_images(image_files).to(device)\n",
    "print(f\"Type: {type(images)}\")\n",
    "print(f\"Shape: {images.shape if hasattr(images, 'shape') else 'No shape'}\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(dtype=dtype):\n",
    "        images = images[None]\n",
    "        aggregated_tokens_list, ps_idx = model.aggregator(images)\n",
    "                \n",
    "    # Predict Cameras\n",
    "    pose_enc = model.camera_head(aggregated_tokens_list)[-1]\n",
    "    # Extrinsic and intrinsic matrices, following OpenCV convention (camera from world)\n",
    "    extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, images.shape[-2:])\n",
    "\n",
    "    # Predict Depth Maps\n",
    "    depth_map, depth_conf = model.depth_head(aggregated_tokens_list, images, ps_idx)\n",
    "\n",
    "    # Predict Point Maps\n",
    "    point_map, point_conf = model.point_head(aggregated_tokens_list, images, ps_idx)\n",
    "        \n",
    "    # Construct 3D Points from Depth Maps and Cameras\n",
    "    # which usually leads to more accurate 3D points than point map branch\n",
    "    point_map_by_unprojection = unproject_depth_map_to_point_map(depth_map.squeeze(0), \n",
    "                                                                extrinsic.squeeze(0), \n",
    "                                                                intrinsic.squeeze(0))\n",
    "\n",
    "    # Predict Tracks\n",
    "    # choose your own points to track, with shape (N, 2) for one scene\n",
    "    query_points = torch.FloatTensor([[100.0, 200.0], \n",
    "                                        [60.72, 259.94]]).to(device)\n",
    "    track_list, vis_score, conf_score = model.track_head(aggregated_tokens_list, images, ps_idx, query_points=query_points[None])\n",
    "\n",
    "    colmap_output_dir = convert_vggt_to_colmap(\n",
    "        extrinsic.squeeze(0),           \n",
    "        intrinsic.squeeze(0),             \n",
    "        point_map_by_unprojection,      \n",
    "        depth_conf.squeeze(0),          \n",
    "        image_files,                    \n",
    "        output_dir=\"./colmap_output\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vggt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
